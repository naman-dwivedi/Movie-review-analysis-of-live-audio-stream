{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentimental analysis of movie reviews:\n",
    "\n",
    "Here for the sentimental analysis of movie reviews the approach is that i have trained a Naive Bayes classifier using nltk movie review corpus that is a part of nltk package itself and for live audio speech to text i have implemented google speech recognition library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we are creating out bag of words which will be used to extract bigrams and unigrams from input speech/text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    " \n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "# clean words, i.e. remove stopwords and punctuation\n",
    "def clean_words(words, stopwords_english):\n",
    "    words_clean = []\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords_english and word not in string.punctuation:\n",
    "            words_clean.append(word)    \n",
    "    return words_clean \n",
    " \n",
    " \n",
    "\n",
    " \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction function for unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extractor function for unigram\n",
    "def bag_of_words(words):    \n",
    "    words_dictionary = dict([word, True] for word in words)    \n",
    "    return words_dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction function for ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_ngrams(words, n=2):\n",
    "    words_ng = []\n",
    "    for item in iter(ngrams(words, n)):\n",
    "        words_ng.append(item)\n",
    "    words_dictionary = dict([word, True] for word in words_ng)    \n",
    "    return words_dictionary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams ['it', 'was', 'a', 'not', 'very', 'good', 'movie', '.']\n",
      "{('it', 'was'): True, ('was', 'a'): True, ('a', 'not'): True, ('not', 'very'): True, ('very', 'good'): True, ('good', 'movie'): True, ('movie', '.'): True}\n",
      "['good', 'movie']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"It was a not very good movie.\"\n",
    "words = word_tokenize(text.lower())\n",
    " \n",
    "print (\"Unigrams\", words)\n",
    " \n",
    "print (bag_of_ngrams(words))\n",
    " \n",
    "words_clean = clean_words(words, stopwords_english)\n",
    "print (words_clean)\n",
    " \n",
    "important_words = ['above', 'below', 'off', 'over', 'under', 'more', 'not', 'most', 'such', 'no', 'nor', 'only', 'so', 'than', 'too', 'very', 'just', 'but']\n",
    " \n",
    "stopwords_english_for_bigrams = set(stopwords_english) - set(important_words)\n",
    " \n",
    "words_clean_for_bigrams = clean_words(words, stopwords_english_for_bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'good': True, 'movie': True}\n",
      "{('not', 'very'): True, ('very', 'good'): True, ('good', 'movie'): True}\n"
     ]
    }
   ],
   "source": [
    "# We will use general stopwords for unigrams \n",
    "# And special stopwords list for bigrams\n",
    "unigram_features = bag_of_words(words_clean)\n",
    "print (unigram_features)\n",
    " \n",
    "bigram_features = bag_of_ngrams(words_clean_for_bigrams)\n",
    "print (bigram_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features are  as follows {'good': True, 'movie': True, ('not', 'very'): True, ('very', 'good'): True, ('good', 'movie'): True}\n",
      "{'not': True, 'very': True, 'good': True, 'movie': True, ('not', 'very'): True, ('very', 'good'): True, ('good', 'movie'): True}\n"
     ]
    }
   ],
   "source": [
    "# combine both unigram and bigram features\n",
    "all_features = unigram_features.copy()\n",
    "all_features.update(bigram_features)\n",
    "print (\"All features are  as follows\", all_features)\n",
    " \n",
    "# let's define a new function that extracts all features\n",
    "# i.e. that extracts both unigram and bigrams features\n",
    "def bag_of_all_words(words, n=2):\n",
    "    words_clean = clean_words(words, stopwords_english)\n",
    "    words_clean_for_bigrams = clean_words(words, stopwords_english_for_bigrams)\n",
    " \n",
    "    unigram_features = bag_of_words(words_clean_for_bigrams)\n",
    "    bigram_features = bag_of_ngrams(words_clean_for_bigrams)\n",
    " \n",
    "    all_features = unigram_features.copy()\n",
    "    all_features.update(bigram_features)\n",
    " \n",
    "    return all_features\n",
    " \n",
    "print (bag_of_all_words(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Now lets prepare our dataset i.e. movie review corpus here for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews \n",
    " \n",
    "pos_reviews = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    words = movie_reviews.words(fileid)\n",
    "    pos_reviews.append(words)\n",
    " \n",
    "neg_reviews = []\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    words = movie_reviews.words(fileid)\n",
    "    neg_reviews.append(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here i have implemented  bag of words to reviews provided by both sets pos and neg respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_reviews_set = []\n",
    "for words in pos_reviews:\n",
    "    pos_reviews_set.append((bag_of_all_words(words), 'Review is positive'))\n",
    " \n",
    "# negative reviews feature set\n",
    "neg_reviews_set = []\n",
    "for words in neg_reviews:\n",
    "    neg_reviews_set.append((bag_of_all_words(words), 'Review is negative'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating test and training sests.\n",
    "Here the data for testing and training is being divided and then shuffle is used for output accuracy to be different everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n",
      "400 1800\n"
     ]
    }
   ],
   "source": [
    "print (len(pos_reviews_set), len(neg_reviews_set)) # Output: (1000, 1000)\n",
    " \n",
    "\n",
    "from random import shuffle \n",
    "shuffle(pos_reviews_set)\n",
    "shuffle(neg_reviews_set)\n",
    " \n",
    "test_set = pos_reviews_set[:200] + neg_reviews_set[:200]\n",
    "train_set = pos_reviews_set[100:] + neg_reviews_set[100:]\n",
    " \n",
    "print(len(test_set),  len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This part of code trains the classifier and then print the accuracy gained(which can be different evertime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.915\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    " \n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voice input\n",
    "Here we are first taking input in the formm of live audio stream and then converting it into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please speak your review for the movie...\n",
      "Finished recording converting input into text\n",
      "You said: I did not like the movie it was not good\n"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "import pyaudio\n",
    "from os import path\n",
    "from pydub import AudioSegment\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import os\n",
    "import re\n",
    "from werkzeug.utils import secure_filename\n",
    "from flask import send_from_directory\n",
    "from werkzeug.utils import secure_filename\n",
    "import speech_recognition as sr\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#from sklearn.feature_extraction.text import CountVectorizer   \n",
    "\n",
    "            \n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 2\n",
    "RATE = 44100\n",
    "CHUNK = 1024\n",
    "RECORD_SECONDS = 15    #set time for audio input\n",
    "WAVE_OUTPUT_FILENAME = \"file.wav\"\n",
    " \n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    " \n",
    "r = sr.Recognizer() #recognizer instance\n",
    "\n",
    "# this will start Recording\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "    rate=RATE, input=True,\n",
    "    frames_per_buffer=CHUNK)\n",
    "print (\"Please speak your review for the movie...\")\n",
    "frames = []\n",
    " \n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "print (\"Finished recording converting input into text\")\n",
    " \n",
    " \n",
    "#this part stops recording\n",
    "stream.stop_stream(     )\n",
    "stream.close()\n",
    "audio.terminate()\n",
    " \n",
    "waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "waveFile.setnchannels(CHANNELS)\n",
    "waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "waveFile.setframerate(RATE)\n",
    "waveFile.writeframes(b''.join(frames))\n",
    "waveFile.close()\n",
    "\n",
    "src = \"file.wav\"\n",
    "dst = \"audio.wav\"\n",
    "sound = AudioSegment.from_mp3(src)\n",
    "audio = sound.export(dst, format=\"wav\")\n",
    "\n",
    "#parsing the audio file in speech recognizer\n",
    "with sr.AudioFile(\"audio.wav\") as source:\n",
    "    #audio_text = r.record(source)\n",
    "    audio = r.adjust_for_ambient_noise(source)\n",
    "    audio_in  = r.listen(source)\n",
    "    str = r.recognize_google(audio_in)\n",
    "    try:\n",
    "        print(\"You said: \" + str)   \n",
    "    except LookupError:                                 \n",
    "        print(\"Could not understand your review\")\n",
    "    \n",
    "                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review analysis.\n",
    "Review analysis is done in this part here we are passing our input audio in the form of string(str) and then applying out earlier defined function bag of words to this customised review so the classifier will judge the review as its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You said: I did not like the movie it was not good\n",
      "{'not': True, 'like': True, 'movie': True, 'good': True, ('not', 'like'): True, ('like', 'movie'): True, ('movie', 'not'): True, ('not', 'good'): True}\n",
      "Review is negative\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "custom_review = str\n",
    "print(\"You said:\", custom_review)\n",
    "custom_review_tokens = word_tokenize(custom_review)\n",
    "custom_review_set = bag_of_all_words(custom_review_tokens)\n",
    "print(custom_review_set)\n",
    "output= classifier.classify(custom_review_set)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
